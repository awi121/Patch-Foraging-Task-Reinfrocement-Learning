{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QyvZgbFLVsdS"
   },
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "051e8a80"
   },
   "outputs": [],
   "source": [
    "dist=[0,1,1.848,2.414,2.613,2.414,1.848,1]\n",
    "def timer(a1,a2):\n",
    "    return dist[abs(a1-a2)]\n",
    "r=[70,0,0,70,70,0,70,0]\n",
    "rr=[8, 0, 0, 2, 2 , 0, 8, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "b1ab8341"
   },
   "outputs": [],
   "source": [
    "class forage_env(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(8)\n",
    "        # Temperature array\n",
    "        self.observation_space = Discrete(8)\n",
    "        self.state=random.randint(0,7)\n",
    "        # Total time per episode\n",
    "        self.timer = 300\n",
    "        self.rewards=[70,0,0,70,70,0,70,0]\n",
    "        self.reward_rate=[8, 0, 0, 2, 2 , 0, 8, 0]\n",
    "        self.E=[0,0,0,0,0,0,0,0]\n",
    "        self.F=[]\n",
    "        self.G=[]\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.F.append(action)\n",
    "        a=self.state\n",
    "        self.state = action\n",
    "        self.timer-=1+timer(a,action)\n",
    "        #print(self.timer)\n",
    "        reward=self.rewards[action]\n",
    "        for i in range(len(self.rewards)):\n",
    "            if(i!=action):\n",
    "                if(self.reward_rate[i]+self.rewards[i]>200):\n",
    "                    self.rewards[i]=200\n",
    "                else:   \n",
    "                    self.rewards[i]=self.reward_rate[i]+self.rewards[i]\n",
    "            else:\n",
    "                self.rewards[i]=0.9*self.rewards[i]\n",
    "                self.E[i]+=1\n",
    "        \n",
    "        if self.timer <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "        self.G.append(reward)\n",
    "        #print(self.E)\n",
    "        #print(reward)\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state=random.randint(0,7)\n",
    "        self.timer = 300\n",
    "        self.rewards=[70,0,0,70,70,0,70,0]\n",
    "        self.E=[0,0,0,0,0,0,0,0]\n",
    "        self.F=[]\n",
    "        self.G=[]\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "t4ollWnYP6Lk"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = forage_env()\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "        #self.actioncount=[]\n",
    "\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        return old_state, action, reward, new_state\n",
    "\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_v = r + GAMMA * best_v\n",
    "        old_v = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_v * (1-ALPHA) + new_v * ALPHA\n",
    "        #print(self.values)\n",
    "        #return self.values[(s, a)]\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            #self.actioncount.append(action)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCrKBOMBS_X-",
    "outputId": "7621015a-8f52-424b-e12c-4a83a4187276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 700.000\n",
      "Best reward updated 700.000 -> 5980.007\n",
      "Best reward updated 5980.007 -> 7789.067\n",
      "Best reward updated 7789.067 -> 8360.451\n",
      "Best reward updated 8360.451 -> 8509.825\n",
      "Best reward updated 8509.825 -> 9890.361\n",
      "Best reward updated 9890.361 -> 9989.216\n",
      "Best reward updated 9989.216 -> 10046.927\n",
      "Best reward updated 10046.927 -> 10402.234\n",
      "Best reward updated 10402.234 -> 10443.333\n",
      "Best reward updated 10443.333 -> 10489.144\n",
      "Best reward updated 10489.144 -> 10501.479\n",
      "Best reward updated 10501.479 -> 12964.899\n",
      "Best reward updated 12964.899 -> 13110.348\n",
      "Best reward updated 13110.348 -> 13164.899\n"
     ]
    }
   ],
   "source": [
    "step_counter=[]\n",
    "step_=[]\n",
    "reward_=[]\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "GAMMA = 1\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 1\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = forage_env()\n",
    "    agent = Agent()\n",
    "    #writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while iter_no<10000:\n",
    "        iter_no += 1\n",
    "        s, a, r, next_s = agent.sample_env()\n",
    "        agent.value_update(s, a, r, next_s)\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "            \n",
    "        reward /= TEST_EPISODES\n",
    "        #writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "                best_reward, reward))\n",
    "            #print(agent.values)\n",
    "            \n",
    "            step_counter.append(test_env.E)\n",
    "            step_.append(test_env.F)\n",
    "            reward_.append(test_env.G)\n",
    "            best_reward = reward\n",
    "            \n",
    "    #writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[299, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 75, 75],\n",
       " [0, 0, 25, 25, 25, 1, 26, 0],\n",
       " [53, 0, 0, 0, 0, 0, 53, 0],\n",
       " [52, 0, 0, 1, 0, 0, 53, 0],\n",
       " [31, 0, 0, 31, 0, 0, 31, 0],\n",
       " [31, 0, 0, 31, 0, 0, 32, 0],\n",
       " [32, 0, 0, 31, 0, 0, 31, 0],\n",
       " [33, 0, 0, 0, 32, 0, 32, 0],\n",
       " [33, 0, 0, 0, 33, 0, 32, 0],\n",
       " [32, 0, 0, 1, 32, 0, 32, 0],\n",
       " [33, 0, 0, 0, 32, 0, 33, 0],\n",
       " [27, 0, 0, 27, 27, 0, 27, 0],\n",
       " [27, 0, 0, 27, 28, 0, 27, 0],\n",
       " [27, 0, 0, 27, 27, 0, 28, 0]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_counter            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_[8]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70,\n",
       " 72,\n",
       " 86,\n",
       " 79.0,\n",
       " 68.8,\n",
       " 93.4,\n",
       " 87.10000000000001,\n",
       " 65.92,\n",
       " 100.06,\n",
       " 94.39000000000001,\n",
       " 63.328,\n",
       " 106.054,\n",
       " 100.95100000000002,\n",
       " 60.995200000000004,\n",
       " 111.4486,\n",
       " 106.85590000000002,\n",
       " 58.895680000000006,\n",
       " 116.30374,\n",
       " 112.17031000000001,\n",
       " 57.00611200000001,\n",
       " 120.673366,\n",
       " 116.95327900000001,\n",
       " 55.30550080000001,\n",
       " 124.6060294,\n",
       " 121.25795110000001,\n",
       " 53.774950720000014,\n",
       " 128.14542646,\n",
       " 125.13215599000002,\n",
       " 52.39745564800001,\n",
       " 131.330883814,\n",
       " 128.61894039100002,\n",
       " 51.15771008320001,\n",
       " 134.1977954326,\n",
       " 131.75704635190004,\n",
       " 50.041939074880005,\n",
       " 136.77801588934,\n",
       " 134.58134171671003,\n",
       " 49.037745167392,\n",
       " 139.10021430040598,\n",
       " 137.123207545039,\n",
       " 48.133970650652806,\n",
       " 141.19019287036537,\n",
       " 139.41088679053513,\n",
       " 47.320573585587525,\n",
       " 143.07117358332883,\n",
       " 141.46979811148162,\n",
       " 46.58851622702878,\n",
       " 144.76405622499595,\n",
       " 143.32281830033347,\n",
       " 45.9296646043259,\n",
       " 146.28765060249637,\n",
       " 144.99053647030013,\n",
       " 45.33669814389331,\n",
       " 147.65888554224674,\n",
       " 146.49148282327013,\n",
       " 44.80302832950398,\n",
       " 148.89299698802208,\n",
       " 147.8423345409431,\n",
       " 44.32272549655358,\n",
       " 150.00369728921987,\n",
       " 149.0581010868488,\n",
       " 43.89045294689822,\n",
       " 151.0033275602979,\n",
       " 150.15229097816393,\n",
       " 43.5014076522084,\n",
       " 151.9029948042681,\n",
       " 151.13706188034755,\n",
       " 43.151266886987564,\n",
       " 152.7126953238413,\n",
       " 152.0233556923128,\n",
       " 42.83614019828881,\n",
       " 153.44142579145716,\n",
       " 152.82102012308152,\n",
       " 42.55252617845993,\n",
       " 154.09728321231145,\n",
       " 153.53891811077338,\n",
       " 42.29727356061394,\n",
       " 154.6875548910803,\n",
       " 154.18502629969603,\n",
       " 42.06754620455254,\n",
       " 155.2187994019723,\n",
       " 154.76652366972644,\n",
       " 41.86079158409729,\n",
       " 155.69691946177505,\n",
       " 155.2898713027538,\n",
       " 41.67471242568757,\n",
       " 156.12722751559755,\n",
       " 155.76088417247843,\n",
       " 41.50724118311881,\n",
       " 156.5145047640378,\n",
       " 156.1847957552306,\n",
       " 41.35651706480693,\n",
       " 156.86305428763404,\n",
       " 156.56631617970754,\n",
       " 41.220865358326236,\n",
       " 157.17674885887064,\n",
       " 156.90968456173678]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Qlearning_book.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
